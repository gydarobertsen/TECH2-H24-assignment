
To conclude the assignment that I was given concerning creating a program with Python in Spyder to compute the standard deviation of a list of numbers, using three different approaches. 

Furthermore, I was assigned the task of designing a code in Jupyter Notebook to verify which approach was the quickest, and whether or not this varies with the length of the list. 

The findings clearly state that when iterating over 100 values, the quickest method would be to use for-loops, taking only 5.25 microseconds. It varies with as little as 11 nanoseconds per loop. 

Moreover, when computing with 1000 values in a list, it is clearly more time efficient to utilise the function np.std() from Python's library NumPy to calculate the standard deviation. The program states that for 1000 values, NumPy's function uses 30.1 microseconds for computations. 

Finally, for computations with lists of 10,000 values the program clearly shows that NumPy's function is the most efficient, utilising only 235 microseconds, compared to 518 microseconds using for-loops and 1.01 milliseconds using the built-in functions. However, with this amount of values in the list, NumPy's function has as much as 9.94 microseconds of variance.

In conclusion, it is more efficient to use for-loops for computing the standard deviation for shorter lists. On the other hand, the longer the lists get, the more time efficient it will get to use NumPy's respective function to calculate the standard deviation. In addition, I will comment that the variance in time in general is negligible, because of the unit of measurement. Given that a nanosecond is no more than one thousandth of a microsecond, it is usually less significant. 

Lastly, I would like to comment that the output of the code is not consistent with every run of the program. However, the result of the experiment stays the same. The amount of variance usually stays with the decimals. I am confident in the conclusion. 